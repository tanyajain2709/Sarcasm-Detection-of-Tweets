{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(\"/home/nile\")\n",
    "os.curdir\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME']='/home/nile/spark-2.1.0-bin-hadoop2.7'\n",
    "SPARK_HOME=os.environ['SPARK_HOME']\n",
    "\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.4-src.zip\"))\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.executor.memory\",\"1g\")\n",
    "conf.set(\"spark.cores.max\",\"1\")\n",
    "\n",
    "conf.setAppName(\"major\")\n",
    "\n",
    "sc = SparkContext('local',conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "import nltk.corpus\n",
    "import string\n",
    "import re , math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#slang dict creation\n",
    "f = open('slang.txt', 'r')\n",
    "slang_dict = {}\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    parts = [p.strip() for p in line.split(\"=\")]\n",
    "    slang_dict[parts[0].lower()] = (parts[1].lower())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive = list()\n",
    "f = open('/home/nile/drive/major/major2/codes/positive.txt', 'r')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    positive.append(line)\n",
    "f.close()\n",
    "\n",
    "negative = list()\n",
    "f = open('/home/nile/drive/major/major2/codes/negative.txt', 'r')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    negative.append(line)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "import editdistance \n",
    "d = enchant.Dict(\"en_US\")\n",
    "sqlContext = SQLContext(sc)\n",
    "import re ,string\n",
    "def preprocess(x):\n",
    "    l = x.split(',')\n",
    "    if(len(l)>=4):\n",
    "        #convert to lower case\n",
    "        text = str(l[3].decode('utf-8')).lower()\n",
    "        text=text.replace('\"text\":', '')\n",
    "        \n",
    "        #2)standardising slang words\n",
    "        \n",
    "        l1 = text.split(\" \")\n",
    "        if '' in l1:\n",
    "            l1.remove('')\n",
    "        for i in range(len(l1)):\n",
    "            if l1[i] in slang_dict:\n",
    "                l1[i] = slang_dict[l1[i]]\n",
    "        text=' '.join(l1)\n",
    "        \n",
    "        #covert @user to at_user\n",
    "        text= re.sub('@[^\\s]+','atuser',text)\n",
    "        \n",
    "        #covert #data to data\n",
    "        #text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "        \n",
    "        #Remove additional white spaces\n",
    "        text = re.sub('[\\s]+', ' ', text)\n",
    "        \n",
    "        text  = text.replace('\\\\', '')\n",
    "        #replace url by 'url'\n",
    "        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n",
    "        \n",
    "         #--feature vector--#\n",
    "\n",
    "        #1)removing punctuation marks\n",
    "        text =  text.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "         #3) removing repeated chars\n",
    "        pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "        text = pattern.sub(r\"\\1\\1\", text)\n",
    "        \n",
    "        #3) removing words containing no\n",
    "        text=re.sub(r'\\w*\\d\\w*', '', text).strip()\n",
    "       \n",
    "                \n",
    "        \n",
    "        sid = list(str(l[1]))[5:]\n",
    "        sid = int(''.join(sid))\n",
    "        \n",
    "        return Row(tweet=str(text),tid=sid,status=0)\n",
    "        \n",
    "    else:\n",
    "        return Row(tweet=\"not there\",tid=0,status=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31522"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.textFile('/home/nile/drive/major/major2/codes/combined_new.txt')\n",
    "rdd1 = rdd1.map(preprocess)\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10855"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddTrump = sc.textFile('/home/nile/drive/major/major2/codes/dataset-trump/final_dataset.txt')\n",
    "rddTrump = rddTrump.map(preprocess)\n",
    "rddTrump.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14542"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove the undesired tweets\n",
    "def rem_null(x):\n",
    "    tweet = x[2]\n",
    "    if x[2] != \"not there\" and len(tweet.split(' '))>3:\n",
    "        return x\n",
    "\n",
    "rdd1 = rdd1.filter(rem_null)\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5213"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddTrump = rddTrump.filter(rem_null)\n",
    "rddTrump.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_word(tweet,word):\n",
    "    t_list = tweet.split(' ')\n",
    "    w_list = word.split(' ')\n",
    "    l = len(w_list)\n",
    "    for i in range(0,len(t_list)-l+1):\n",
    "        if t_list[i:i+l] == w_list:\n",
    "            return 1\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "    return 0\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### Rules for feature extraction ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_ordered_sar(tweet):\n",
    "    t_list = tweet.split(' ')\n",
    "    for term in positive: \n",
    "        w_list = term.split(' ')\n",
    "        l = len(w_list)\n",
    "        for i in range(0,len(t_list)-l+1):\n",
    "            if t_list[i:i+l] == w_list:\n",
    "                start = i+1\n",
    "                for term in negative:\n",
    "                    if(check_word(' '.join(t_list[i+1:]),term) == 1):\n",
    "                        return 1\n",
    "            else:\n",
    "                pass\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_ordered_sar(x):\n",
    "    tweet = x[2]\n",
    "    stat = x[0]\n",
    "    sid = x[1]\n",
    "    if(check_ordered_sar(tweet)==1):\n",
    "        stat = 1\n",
    "    return(Row(status=stat,tid=sid,tweet=tweet))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = rdd1.map(filter_ordered_sar)\n",
    "final.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_list = []\n",
    "for x in final.collect():\n",
    "    tweet = x[2]\n",
    "    tweet_list.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump = rddTrump.map(filter_ordered_sar)\n",
    "for x in trump.collect():\n",
    "    tweet_list.append(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "cvector = CountVectorizer()\n",
    "final_cvector = cvector.fit_transform(tweet_list)\n",
    "tf_idf = TfidfTransformer(use_idf=False).fit(final_cvector)\n",
    "train = tf_idf.transform(final_cvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureVector = train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelVector = []\n",
    "for x in final.collect():\n",
    "    status = x[0]\n",
    "    labelVector.append(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in trump.collect():\n",
    "    labelVector.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### Naive Bayes ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelVector = np.array(labelVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = featureVector[0:14542:1]\n",
    "label = labelVector[0:14542:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(features,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/nile/drive/major/major2/codes/models/naive_bayes_classifier.sav', 'wb') as fid:\n",
    "    pickle.dump(naive_bayes, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/home/nile/drive/major/major2/codes/models/naive_bayes_classifier.sav', 'rb') as fid:\n",
    "     nb_loaded = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_vector = featureVector[14543:19755:1]\n",
    "test_label = labelVector[14543:19755:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_loaded.predict(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### LOGISTIC REGRESSION ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(random_state=1)\n",
    "logistic.fit(features,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/nile/drive/major/major2/codes/models/logistic.sav','wb') as fp:\n",
    "    pickle.dump(logistic,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.predict(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## RANDOM FOREST ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=1,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(random_state=1)\n",
    "random_forest.fit(features,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/nile/drive/major/major2/codes/models/random_forest.sav','wb') as fp:\n",
    "    pickle.dump(random_forest,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.predict(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/nile/drive/major/major2/codes/models/logistic.sav', 'rb') as fid:\n",
    "     lr_loaded = pickle.load(fid)\n",
    "        \n",
    "with open('/home/nile/drive/major/major2/codes/models/random_forest.sav', 'rb') as fid:\n",
    "     rf_loaded = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54 (+/- 0.04) [naive Bayes]\n",
      "Accuracy: 0.86 (+/- 0.01) [logistic regression]\n",
      "Accuracy: 0.85 (+/- 0.01) [random forest]\n"
     ]
    }
   ],
   "source": [
    "weights = list()\n",
    "accuracy = list()\n",
    "for cl, label1 in zip([nb_loaded, lr_loaded,rf_loaded], ['naive Bayes','logistic regression','random forest']): \n",
    "    scores = cross_validation.cross_val_score(cl, features, label, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label1))\n",
    "    weights.append(scores.mean())\n",
    "    accuracy.append(scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index in range(len(weights)):\n",
    "    weights[index] = int(weights[index]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nb', GaussianNB(priors=None)), ('logistic regression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "        ...estimators=10, n_jobs=1, oob_score=False, random_state=1,\n",
       "            verbose=0, warm_start=False))],\n",
       "         n_jobs=1, voting='hard', weights=[5, 8, 8])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted = VotingClassifier(estimators=[\n",
    "        ('nb', nb_loaded), ('logistic regression', lr_loaded), ('random forest', rf_loaded)], voting='hard',weights=weights)\n",
    "weighted.fit(features,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/nile/drive/major/major2/codes/models/weighted.sav','wb') as fp:\n",
    "    pickle.dump(weighted,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/nile/drive/major/major2/codes/models/weighted.sav', 'rb') as fid:\n",
    "     we_loaded = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision = list()\n",
    "recall = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85 (+/- 0.01) [weighted ensemble]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validation.cross_val_score(we_loaded, features, label, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), \"weighted ensemble\"))\n",
    "accuracy.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54256900031669952, 0.85531478514064774, 0.84747590483888513, 0.85290834148716432]\n"
     ]
    }
   ],
   "source": [
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.21 (+/- 0.02) [naive Bayes]\n",
      "precision: 0.83 (+/- 0.05) [logistic regression]\n",
      "precision: 0.84 (+/- 0.04) [random forest]\n",
      "precision: 0.82 (+/- 0.03) [weighted ensemble]\n"
     ]
    }
   ],
   "source": [
    "for cl, label1 in zip([nb_loaded,lr_loaded,rf_loaded,we_loaded], ['naive Bayes','logistic regression','random forest','weighted ensemble']):\n",
    "    scores = cross_validation.cross_val_score(cl, features, label, cv=5, scoring='precision')\n",
    "    print(\"precision: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label1))\n",
    "    precision.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21416185158943496, 0.83130615690263665, 0.83827543259522963, 0.81516453855412363]\n"
     ]
    }
   ],
   "source": [
    "print precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.53 (+/- 0.02) [naive Bayes]\n",
      "recall: 0.29 (+/- 0.06) [logistic regression]\n",
      "recall: 0.24 (+/- 0.04) [random forest]\n",
      "recall: 0.28 (+/- 0.04) [weighted ensemble]\n"
     ]
    }
   ],
   "source": [
    "for cl, label1 in zip([nb_loaded,lr_loaded,rf_loaded,we_loaded], ['naive Bayes','logistic regression','random forest','weighted ensemble']):\n",
    "    scores = cross_validation.cross_val_score(cl, features, label, cv=5, scoring='recall')\n",
    "    print(\"recall: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label1))\n",
    "    recall.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21416185158943496, 0.83130615690263665, 0.83827543259522963, 0.81516453855412363]\n",
      "[0.5316846468583214, 0.29070856017839564, 0.23506170855348185, 0.28083318266133628]\n",
      "[0.54256900031669952, 0.85531478514064774, 0.84747590483888513, 0.85290834148716432]\n"
     ]
    }
   ],
   "source": [
    "print precision\n",
    "print recall\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/nile/drive/major/major2/codes/results/precision.txt','wb') as fp:\n",
    "    pickle.dump(precision,fp)\n",
    "\n",
    "with open('/home/nile/drive/major/major2/codes/results/accuracy.txt','wb') as fp:\n",
    "    pickle.dump(accuracy,fp)\n",
    "    \n",
    "with open('/home/nile/drive/major/major2/codes/results/recall.txt','wb') as fp:\n",
    "    pickle.dump(recall,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
